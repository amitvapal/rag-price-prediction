205_week8_day2_JustCode_test_RAG_pipeline.ipynb



## 1. **PIP Installations**

# Install PyTorch and transformers
!pip install torch transformers accelerate huggingface_hub bitsandbytes

# Install RAPIDS libraries for GPU
!pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com
!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com

# Install ChromaDB and Items
!pip install -q chromadb Items





## 2. **GPU and System Checks**
import torch

# Check CUDA version
!nvcc --version

print("="*70)
print("GPU VERIFICATION")
print("="*70)
print(f"CUDA Available: {torch.cuda.is_available()}")
assert torch.cuda.is_available(), "‚ùå CUDA GPU not found! Change runtime to GPU."

print(f"GPU Device: {torch.cuda.get_device_name(0)}")
print(f"Current Device: {torch.cuda.current_device()}")
print(f"Device Count: {torch.cuda.device_count()}")

gpu_name = torch.cuda.get_device_name(0)
if "L4" not in gpu_name:
    print(f"‚ö†Ô∏è Non-L4 GPU detected: {gpu_name}")
else:
    print(f"‚úì Using L4 GPU: {gpu_name}")
print("="*70)

# Set device
device = torch.device('cuda')





## 3. **HuggingFace Authentication**

from google.colab import userdata
import os
from huggingface_hub import login, whoami

# Get token from Colab secrets
hf_token = userdata.get('HF_TOKEN')

# Set environment variable
os.environ["HF_TOKEN"] = hf_token

# Log in to Hugging Face
login(token=hf_token, add_to_git_credential=True)

# Confirm login worked
print(whoami())




## 4. **Mount Google Drive**
from google.colab import drive

# Unmount if already mounted
drive.flush_and_unmount()

# Remount
drive.mount('/content/drive', force_remount=True)

# Navigate to your folder
os.chdir('/content/drive/MyDrive/01-LLM-ed-donner/build_multi_agent_system')

# Get current working directory
print(f"Current directory: {os.getcwd()}")

# List contents
print("\nContents:")
for item in os.listdir('.'):
    print(f"  üìÅ {item}" if os.path.isdir(item) else f"  üìÑ {item}")

# Full path example
file_path = os.path.join(os.getcwd(), '202_week8_day2_CUDA-code-chroma_vector_db_advanced_rag_pipeline.ipynb')
print(f"Full path: {file_path}")



## 5. **Main Imports**

# ===================================================================
# STEP 1: Standard library imports
# ===================================================================
import sys
import os
import re
import math
import json
import random
from datetime import datetime
import pickle
import time

# ===================================================================
# STEP 2: Third-party imports
# ===================================================================
from tqdm import tqdm
from dotenv import load_dotenv

import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

import chromadb

from sklearn.manifold import TSNE
import numpy as np
import plotly.graph_objects as go

# ===================================================================
# STEP 3: Setup Python path BEFORE importing custom modules
# ===================================================================
base_path = '/content/drive/MyDrive/01-LLM-ed-donner/build_multi_agent_system'

# Change to base directory
os.chdir(base_path)

# Add scripts folder to Python path
scripts_path = os.path.join(base_path, 'scripts')
if scripts_path not in sys.path:
    sys.path.insert(0, scripts_path)

print("="*70)
print("PATH SETUP")
print("="*70)
print(f"Current directory: {os.getcwd()}")
print(f"Scripts path added: {scripts_path}")
print(f"Scripts folder exists: {os.path.exists(scripts_path)}")

# List files in scripts folder to verify
if os.path.exists(scripts_path):
    print("\nFiles in scripts folder:")
    for file in os.listdir(scripts_path):
        print(f"  - {file}")

print("="*70)

# ===================================================================
# STEP 4: NOW import custom modules (after path is configured)
# ===================================================================
try:
    from preprocess_data import Item
    print("‚úì Successfully imported Item from preprocess_data")
except ImportError as e:
    print(f"‚ùå Failed to import Item: {e}")
    # Try alternative import
    try:
        from items import Item
        print("‚úì Successfully imported Item from items")
    except ImportError as e2:
        print(f"‚ùå Failed to import from items too: {e2}")

try:
    from testing import Tester
    print("‚úì Successfully imported Tester from testing")
except ImportError as e:
    print(f"‚ùå Failed to import Tester: {e}")

print("\n‚úì All imports configured successfully")




## 6. **Load LLaMA Model**
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "meta-llama/Meta-Llama-3.1-8B"

print("="*70)
print("LOADING LLAMA MODEL ON GPU")
print("="*70)

# Load tokenizer
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model with GPU optimization
print("Loading model to GPU with FP16 precision...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # Automatically uses GPU
    torch_dtype=torch.float16,  # Use FP16 for 2x faster inference
    low_cpu_mem_usage=True
).eval()  # Set to evaluation mode

print(f"‚úì Model loaded on: {model.device}")
print(f"‚úì Model dtype: {model.dtype}")
print(f"‚úì Model is in eval mode: {not model.training}")
print("="*70)



## 7. **Test Model**

# Set the Llama 3 chat template
tokenizer.chat_template = "{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}"

messages = [{"role": "user", "content": "What is the capital of France?"}]

# Apply chat template
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# Tokenize and move to GPU
inputs = tokenizer(prompt, return_tensors="pt").to(device)

print(f"Input tensor device: {inputs['input_ids'].device}")

# Generate with GPU
start_time = time.time()
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=10,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
inference_time = time.time() - start_time

# Decode output
input_length = inputs['input_ids'].shape[1]
generated_ids = outputs[0][input_length:]
full_answer = tokenizer.decode(generated_ids, skip_special_tokens=True)

answer = full_answer.split('\n')[0].strip()
print(f"Answer: {answer}")
print(f"Inference time: {inference_time:.3f} seconds")
print(f"‚úì GPU inference working correctly")


## 8. **Copy Dataset to Colab VM**
print("="*70)
print("COPYING DATASET TO LOCAL VM")
print("="*70)

# Define paths
drive_data_folder = "/content/drive/MyDrive/01-LLM-ed-donner/build_multi_agent_system/data/final_v_02"
local_data_folder = "/content/final_data"

# Copy from Drive to local disk for faster access
print(f"Copying from Drive to {local_data_folder}...")
!cp -r "$drive_data_folder" "$local_data_folder"
print("‚úì Copy complete")

# Update data_folder variable
data_folder = local_data_folder

# Load pickle files
print("Loading pickle files...")
with open(os.path.join(data_folder, "train.pkl"), 'rb') as file:
    train = pickle.load(file)

with open(os.path.join(data_folder, "test.pkl"), 'rb') as file:
    test = pickle.load(file)

print(f"‚úì Loaded train: {len(train):,} items")
print(f"‚úì Loaded test: {len(test):,} items")
print("="*70)


## 9. **Copy Vector Database to Colab VM**
print("="*70)
print("COPYING VECTOR DATABASE TO LOCAL VM")
print("="*70)

# Define paths
drive_db_source = "/content/drive/MyDrive/01-LLM-ed-donner/build_multi_agent_system/products_vectorstore"
local_db_path = "/content/products_vectorstore"

# Copy database
print("Copying database from Drive to local disk...")
!cp -r "$drive_db_source" /content/
print("‚úì Copy complete")

# Create ChromaDB client
client = chromadb.PersistentClient(path=local_db_path)
collection = client.get_or_create_collection('products')

print(f"‚úì ChromaDB connected")
print(f"‚úì Collection count: {collection.count():,} items")
print("="*70)



## 10. **Load Sentence Transformer Model**
print("="*70)
print("LOADING SENTENCE TRANSFORMER ON GPU")
print("="*70)

# Force GPU usage
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Loading Sentence Transformer on {device}...")

model_sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2', device=device)

print(f"‚úì Sentence Transformer on: {model_sentence_transformer.device}")
print("="*70)


## 11. **Helper Functions**
# Description extraction
def description(item):
    text = item.prompt.replace("How much does this cost to the nearest dollar?\n\n", "")
    return text.split('\n\nPrice is $')[0]

# Vector generation - GPU optimized
def vector(item):
    with torch.no_grad():
        embedding = model_sentence_transformer.encode(
            [description(item)],
            convert_to_tensor=True,
            device='cuda'
        )
    return embedding.cpu().numpy()  # Convert to numpy for ChromaDB

# Find similar items
def find_similars(item):
    results = collection.query(
        query_embeddings=vector(item).astype(float).tolist(),
        n_results=5
    )
    documents = results['documents'][0][:]
    prices = [m['price'] for m in results['metadatas'][0][:]]
    return documents, prices

# Make context for LLM
def make_context(similars, prices):
    message = "To provide some context, here are some other items that might be similar to the item you need to estimate.\n\n"
    for similar, price in zip(similars, prices):
        message += f"Potentially related product:\n{similar}\nPrice is ${price:.2f}\n\n"
    return message

# Extract price from text
def get_price(s):
    s = s.replace('$','').replace(',','')
    match = re.search(r"[-+]?\d*\.?\d+", s)
    return float(match.group()) if match else 0

# RAG prediction function - GPU optimized
@torch.no_grad()  # Disable gradients for inference
def llama_rag(item):
    # Get similar items from vector DB
    documents, prices = find_similars(item)
    context = make_context(documents, prices)

    # Create prompt
    prompt = f"""You estimate prices. Reply only with the price number.

{context}

How much does this cost?
{description(item)}

Price is $"""

    # Tokenize and move to GPU
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Generate on GPU
    outputs = model.generate(
        **inputs,
        max_new_tokens=10,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        use_cache=True  # Enable KV cache for speed
    )

    # Decode output
    input_length = inputs['input_ids'].shape[1]
    reply = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()

    return get_price(reply)

print("‚úì All helper functions defined")



## 12. **Test RAG Pipeline**
print("="*70)
print("TESTING RAG PIPELINE ON GPU")
print("="*70)



start_time = time.time()
Tester.test(llama_rag, test)
end_time = time.time()

total_time = end_time - start_time
print(f"\n‚è±Ô∏è Processing time: {total_time:.2f} seconds")
print(f"‚è±Ô∏è Time per item: {total_time / len(test_sample):.2f} seconds")
print("="*70)


